# -*- coding: utf-8 -*-
"""Final_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M9VeourumkiAUXV8M5sCsYJ7GDItzV6W
"""

from google.colab import drive
drive.mount("/content/drive", force_remount=True)
FOLDER = "/content/drive/My Drive"
original_en = f"{FOLDER}/news-commentary-v9.fr-en.en"
original_fr = f"{FOLDER}/news-commentary-v9.fr-en.fr"
noisy_en = f"{FOLDER}/noisy_dataset.en"

!pip install sacrebleu
!pip install jiwer

from transformers import AutoTokenizer, T5ForConditionalGeneration, MarianMTModel
import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import torch
import json
import sacrebleu
nltk.download('punkt_tab')
nltk.download('wordnet')
from nltk.translate.meteor_score import meteor_score
from nltk.tokenize import sent_tokenize, word_tokenize

def load_data(filepath: str) -> list[str]:
    with open(filepath, "r") as input_file:
      lines = input_file.readlines()
      result = []
      for line in lines:
        if not line.isspace():
          result.append(line.strip())
      return result

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def apply_model(model_name: str, texts: list[str]) -> list[list[str]]:
  tokenizer = AutoTokenizer.from_pretrained(model_name)
  if model_name == 'Helsinki-NLP/opus-mt-tc-big-en-fr':
    model = MarianMTModel.from_pretrained(model_name).to(device)
    input_texts = texts
  else:
    model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)
    input_texts = [f"translate English to French: {english_sentence}" for english_sentence in texts]

  inputs = tokenizer(input_texts, return_tensors="pt", padding=True, truncation=True)

  input_ids = inputs['input_ids'].to(device)
  attention_mask = inputs['attention_mask'].to(device)

  generated_ids = model.generate(input_ids, attention_mask=attention_mask, max_length=50, num_beams=5, early_stopping=True)

  generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)

  return generated_texts

def calculate_meteor_score(outputs: list[str], references: list[str]) -> float:
  all_scores = []
  for output, reference in zip(outputs, references):
    all_scores.append(meteor_score([word_tokenize(reference)], word_tokenize(output)))
  return sum(all_scores) / len(all_scores)

# Load the data files
original_en_file = load_data(original_en)[:100]
original_fr_file = load_data(original_fr)[:100]
noisy_en_file = load_data(noisy_en)[:100]
print(original_en_file)
print(original_fr_file)
print(noisy_en_file)

reference = original_fr_file
models = ['Helsinki-NLP/opus-mt-tc-big-en-fr', 't5-base', 'google-t5/t5-small']

model_info_map = {}

for model in models:
  original_translation = apply_model(model, original_en_file)
  noisy_translation = apply_model(model, noisy_en_file)
  print(original_translation)
  print(reference)

  original_bleu_score = sacrebleu.corpus_bleu([sentence.strip() for sentence in original_translation], [[sentence.strip()] for sentence in reference], smooth_value=1)
  original_ter_score = sacrebleu.corpus_ter([sentence.strip() for sentence in original_translation], [[sentence.strip()] for sentence in reference])
  original_meteor_score = calculate_meteor_score(original_translation, reference)

  noisy_bleu_score = sacrebleu.corpus_bleu([sentence.strip() for sentence in noisy_translation], [[sentence.strip()] for sentence in reference], smooth_value=1)
  noisy_ter_score = sacrebleu.corpus_ter([sentence.strip() for sentence in noisy_translation], [[sentence.strip()] for sentence in reference])
  noisy_meteor_score = calculate_meteor_score(noisy_translation, reference)

  bleu_score_diff = abs(original_bleu_score.score - noisy_bleu_score.score)
  ter_score_diff = abs(original_ter_score.score - noisy_ter_score.score)
  meteor_score_diff = abs(original_meteor_score - noisy_meteor_score)

  model_info_map[model] = {
      'original_bleu_score': original_bleu_score.score,
      'noisy_bleu_score': noisy_bleu_score.score,
      'original_ter_score': original_ter_score.score,
      'noisy_ter_score': noisy_ter_score.score,
      'original_meteor_score': original_meteor_score,
      'noisy_meteor_score': noisy_meteor_score,
      'bleu_score_diff': bleu_score_diff,
      'ter_score_diff': ter_score_diff,
      'meteor_score_diff': meteor_score_diff,
      'model': model
  }

all_model_info = list(model_info_map.items())
sorted_model_info_by_bleu = sorted(all_model_info, key=lambda x: (x[1]['bleu_score_diff'], x[1]['model']))
sorted_model_info_by_ter = sorted(all_model_info, key=lambda x: (x[1]['ter_score_diff'], x[1]['model']))
sorted_model_info_by_meteor = sorted(all_model_info, key=lambda x: (x[1]['meteor_score_diff'], x[1]['model']))
result_by_bleu, result_by_ter, result_by_meteor = [], [], []

for i, (_, val) in enumerate(sorted_model_info_by_bleu):
  val_copy = val
  val_copy['bleu_id'] = i
  result_by_bleu.append(val_copy)

for i, (_, val) in enumerate(sorted_model_info_by_ter):
  val_copy = val
  val_copy['ter_id'] = i
  result_by_ter.append(val_copy)

for i, (_, val) in enumerate(sorted_model_info_by_meteor):
  val_copy = val
  val_copy['meteor_id'] = i
  result_by_meteor.append(val_copy)

with open(f"{FOLDER}/output.json", 'w') as output_file:
  json_string = json.dumps({
      'result_by_bleu': result_by_bleu,
      'result_by_ter': result_by_ter,
      'result_by_meteor': result_by_meteor
  }, default=lambda o: o.to_dict())
  output_file.write(json_string)